{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Instruction Experiment (LLama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will collect the output for difference examples using the LLama-3.1-8B-Instruct\n",
    "</br>\n",
    "The data used will be the \"reduced_sample\" that consist in a pool of 4 tasks, containing each 40 examples in the training pool and 10 in the test pool\n",
    "</br>\n",
    "The following steps will be followed:\n",
    "- Load datasets\n",
    "- Create Dataloaders\n",
    "- Baseline - Collect outputs\n",
    "- Baeline - Qualitative Analysis\n",
    "- Baseline - Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from src.utils.experiment_samplers import *\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "from src.llms.Llama3_1_Instruct import Llama3_1_Instruct\n",
    "\n",
    "\n",
    "seed = 42\n",
    "# NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../data/bbh_instruction_bias_experiment\"\n",
    "\n",
    "### Default\n",
    "\n",
    "default = \"\"\"\n",
    "Fill the expected Output according to the instruction\n",
    "Intruction: {instruction}\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\"\n",
    "\n",
    "## None\n",
    "none = \"\"\"\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\"\n",
    "\n",
    "### Generic 1\n",
    "generic_1 = \"\"\"\n",
    "You have to fullffil a specific task, it will be given examples that can or not be related to this task.\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\"\n",
    "\n",
    "### Generic 1\n",
    "generic_2 = \"\"\"\n",
    "Use the following examples to answer the User Input correctly, filter the examples in the context\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_ipc(f\"{dataset_path}/train.feather\")\n",
    "test = pl.read_ipc(f\"{dataset_path}/test.feather\")\n",
    "collections_idx = pl.read_ipc(f\"{dataset_path}/collections_idx.feather\")\n",
    "tasks = pl.read_ipc(f\"{dataset_path}/tasks.feather\")\n",
    "collections = pl.read_ipc(f\"{dataset_path}/collections.feather\")\n",
    "instructions = json.load(open(f\"{dataset_path}/instructions.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Datalaoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proportion lists\n",
    "props = [range(1,3), range(2,7), range(7,9)]\n",
    "collections_dls = []\n",
    "for p in props:\n",
    "    collections_dls.append(\n",
    "        create_colletion_dataloaders(\n",
    "            df = collections_idx,\n",
    "            num_tasks = 4,\n",
    "            proportion = p,\n",
    "            batch_size = 5,\n",
    "            shuffle = True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_0': <torch.utils.data.dataloader.DataLoader at 0x7f2d0f612590>,\n",
       "  'task_1': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef85cd0>,\n",
       "  'task_2': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef87ed0>,\n",
       "  'task_3': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef5b990>},\n",
       " {'task_0': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef5ba10>,\n",
       "  'task_1': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef7a8d0>,\n",
       "  'task_2': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef7bed0>,\n",
       "  'task_3': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef79a90>},\n",
       " {'task_0': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef79b50>,\n",
       "  'task_1': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef7a310>,\n",
       "  'task_2': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef786d0>,\n",
       "  'task_3': <torch.utils.data.dataloader.DataLoader at 0x7f2d0ef7be90>}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections_dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_dl = []\n",
    "for t in tasks[\"task\"].to_list():\n",
    "    tests_dl.append(\n",
    "        create_test_dataloader(\n",
    "            df = test,\n",
    "            task = t,\n",
    "            batch_size = 5,\n",
    "            shuffle = True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collect Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [default, none, generic_1, generic_2]\n",
    "\n",
    "def set_instruction_prompt(template, train, train_idxs, test_idx, test, instructions):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    context = \"\"\n",
    "    for idx in train_idxs:\n",
    "            input = train[idx].select(\"input\")\n",
    "            output =  train[idx].select(\"output\")\n",
    "            context += f\"Input: {input} \\nOutput: {output}\\n\"\n",
    "\n",
    "        \n",
    "    input = test[test_idx][\"input\"]\n",
    "\n",
    "    instruction =instructions[test[0].select(\"task\").item()]\n",
    "    prompt = PromptTemplate.from_template(template).format(instruction=instruction, context=context, input=input)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def set_prompt(template, train, train_idxs, test_idx, test):\n",
    "\n",
    "    train = train.to_pandas()\n",
    "    test = test.to_pandas()\n",
    "    \n",
    "    context = \"\"\n",
    "    for idx in train_idxs:\n",
    "            input = train[idx].select(\"input\")\n",
    "            output =  train[idx].select(\"output\")\n",
    "            context += f\"Input: {input} \\nOutput: {output}\\n\"\n",
    "\n",
    "        \n",
    "    input = test[test_idx].select(\"input\")\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template).format(context=context, input=input)\n",
    "\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "18\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m template \u001b[38;5;241m==\u001b[39m default:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(test_idx)\n\u001b[0;32m---> 35\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mset_instruction_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m set_prompt(template, train, train_idxs, test_idx, test)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mset_instruction_prompt\u001b[0;34m(template, train, train_idxs, test_idx, test, instructions)\u001b[0m\n\u001b[1;32m     12\u001b[0m         context \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m test[test_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[43minstructions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\u001b[38;5;241m.\u001b[39mformat(instruction\u001b[38;5;241m=\u001b[39minstruction, context\u001b[38;5;241m=\u001b[39mcontext, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "outputs = {\n",
    "    \"iter\": [],\n",
    "    \"template\": [],\n",
    "    \"test_idx\": [],\n",
    "    \"task\": [],\n",
    "    \"proportion\": [],\n",
    "    \"output\": []\n",
    "}\n",
    "\n",
    "llm = Llama3_1_Instruct()\n",
    "\n",
    "num_iters = 5\n",
    "num_tasks = 4\n",
    "for i in range(num_iters):\n",
    "    print(f\"Iter {i}\")\t\n",
    "    for t in range(num_tasks):\n",
    "\n",
    "        test_dl = tests_dl[t]\n",
    "        test_idxs = next(iter(test_dl))\n",
    "        for test_idx in test_idxs[0]:\n",
    "            test_idx = test_idx[0].item()\n",
    "            for j in range(len(props)):\n",
    "\n",
    "                train_dl = collections_dls[j][\"task_{}\".format(t)]\n",
    "                collections_idxs = next(iter(train_dl))\n",
    "\n",
    "                for c in collections_idx[0]:\n",
    "                    c = c.item()\n",
    "\n",
    "                    train_idxs = collections[c].select(\"indices\").to_numpy()[0][0].tolist()\n",
    "\n",
    "                    for template in templates:\n",
    "                        if template == default:\n",
    "                            prompt = set_instruction_prompt(template, train, train_idxs, test_idx, test, instructions)\n",
    "                        else:\n",
    "                            prompt = set_prompt(template, train, train_idxs, test_idx, test)\n",
    "                        output = llm.run(prompt)\n",
    "\n",
    "                        outputs[\"iter\"].append(i)\n",
    "                        outputs[\"template\"].append(template)\n",
    "                        outputs[\"test_idx\"].append(test_idx)\n",
    "                        outputs[\"task\"].append(t)\n",
    "                        outputs[\"proportion\"].append(j)\n",
    "                        outputs[\"output\"].append(output)\n",
    "                        \n",
    "                        results = pl.DataFrame(outputs).write_ipc(f\"{dataset_path}/results/outputs_llama.feather\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
