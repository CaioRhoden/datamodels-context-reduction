{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Instruction Experiment (LLama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will collect the output for difference examples using the LLama-3.1-8B-Instruct\n",
    "</br>\n",
    "The data used will be the \"reduced_sample\" that consist in a pool of 4 tasks, containing each 40 examples in the training pool and 10 in the test pool\n",
    "</br>\n",
    "The following steps will be followed:\n",
    "- Load datasets\n",
    "- Create Dataloaders\n",
    "- Baseline - Collect outputs\n",
    "- Baeline - Qualitative Analysis\n",
    "- Baseline - Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from src.utils.experiment_samplers import *\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "from src.llms.Llama3_1_Instruct import Llama3_1_Instruct\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 42\n",
    "# NumPy\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../data/bbh_instruction_bias_experiment\"\n",
    "\n",
    "### Default\n",
    "\n",
    "default = \"\"\"\n",
    "Fill the expected Output according to the instruction\n",
    "Intruction: {instruction}\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\"\n",
    "\n",
    "## None\n",
    "none = \"\"\"\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\"\n",
    "\n",
    "### Generic 1\n",
    "generic_1 = \"\"\"\n",
    "You have to fullffil a specific task, it will be given examples that can or not be related to this task.\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\"\n",
    "\n",
    "### Generic 1\n",
    "generic_2 = \"\"\"\n",
    "Use the following examples to answer the User Input correctly, filter the examples in the context\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "Model Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_ipc(f\"{dataset_path}/train.feather\")\n",
    "test = pl.read_ipc(f\"{dataset_path}/test.feather\")\n",
    "collections_idx = pl.read_ipc(f\"{dataset_path}/collections_idx.feather\")\n",
    "tasks = pl.read_ipc(f\"{dataset_path}/tasks.feather\")\n",
    "collections = pl.read_ipc(f\"{dataset_path}/collections.feather\")\n",
    "instructions = json.load(open(f\"{dataset_path}/instructions.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [1, 2,4, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Datalaoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proportion lists\n",
    "# props = [range(1,3), range(2,7), range(7,9)]\n",
    "# collections_dls = []\n",
    "# for p in props:\n",
    "#     collections_dls.append(\n",
    "#         create_colletion_dataloaders(\n",
    "#             df = collections_idx,\n",
    "#             num_tasks = 4,\n",
    "#             proportion = p,\n",
    "#             batch_size = 5,\n",
    "#             shuffle = True\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "proportion_settings = {\n",
    "    1: [((1, 0), \"100%\"), ((0, 1), \"0%\")],\n",
    "    2: [((2, 0), \"100%\"), ((0, 2), \"0%\"), ((1, 1), \"50%\")],\n",
    "    4: [((4, 0), \"100%\"), ((0, 4), \"0%\"), ((2, 2), \"50%\"), ((1, 3), \"25%\")],\n",
    "    8: [((7, 1), \"87.5%\"), ((0, 8), \"0%\"), ((4, 4), \"50%\"), ((2, 6), \"25%\"), ((6, 2), \"75%\"), ((1, 7), \"12.5%\")]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_loaders_1 = create_colletion_dataloaders(\n",
    "    df = collections_idx,\n",
    "    num_tasks = 4,\n",
    "    proportion = range(4,5),\n",
    "    batch_size = 5,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "collection_loaders_2 = create_colletion_dataloaders(\n",
    "    df = collections_idx,\n",
    "    num_tasks = 4,\n",
    "    proportion = range(7,8),\n",
    "    batch_size = 5,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_dl = []\n",
    "for t in tasks[\"task\"].to_list():\n",
    "    tests_dl.append(\n",
    "        create_test_dataloader(\n",
    "            df = test,\n",
    "            task = t,\n",
    "            batch_size = 5,\n",
    "            shuffle = True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collect Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [default, none, generic_1, generic_2]\n",
    "\n",
    "def set_instruction_prompt(template, train, train_idxs, test_idx, test, instructions, num_samples_task, num_samples_general, task):\n",
    "\n",
    "    context = \"\"\n",
    "    task_counter = 0\n",
    "    general_counter = 0\n",
    "\n",
    "    random.shuffle(train_idxs)\n",
    "\n",
    "    for idx in train_idxs:\n",
    "            \n",
    "            if task_counter < num_samples_task and train[idx].select(\"task\").item() == task:\n",
    "                task_counter += 1\n",
    "        \n",
    "\n",
    "            elif general_counter < num_samples_general and train[idx].select(\"task\").item() != task:\n",
    "                general_counter += 1\n",
    "\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            input = train[idx].select(\"input\")\n",
    "            output =  train[idx].select(\"output\")\n",
    "            context += f\"Input: {input} \\nOutput: {output}\\n\"\n",
    "\n",
    "        \n",
    "    input = test[test_idx][\"input\"]\n",
    "\n",
    "    instruction =instructions[test[0].select(\"task\").item()]\n",
    "    prompt = PromptTemplate.from_template(template).format(instruction=instruction, context=context, input=input)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# def set_prompt(template, train, train_idxs, test_idx, test, num_samples_task, num_samples_general, task):\n",
    "    \n",
    "#     task_counter = 0\n",
    "#     general_counter = 0\n",
    "#     context = \"\"\n",
    "\n",
    "#     for idx in train_idxs:\n",
    "            \n",
    "#             if task_counter < num_samples_task and train[idx].select(\"task\").item() == task:\n",
    "#                 task_counter += 1\n",
    "        \n",
    "\n",
    "#             elif general_counter < num_samples_general and train[idx].select(\"task\").item() != task:\n",
    "#                 general_counter += 1\n",
    "\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "        \n",
    "#     input = test[test_idx].select(\"input\")\n",
    "\n",
    "#     prompt = PromptTemplate.from_template(template).format(context=context, input=input)\n",
    "\n",
    "#     return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 Task 0\n",
      "Iter 0 Task 1\n",
      "Iter 0 Task 2\n",
      "Iter 0 Task 3\n",
      "Iter 1 Task 0\n",
      "Iter 1 Task 1\n",
      "Iter 1 Task 2\n",
      "Iter 1 Task 3\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../../data/bbh_sample_proportion\"\n",
    "\n",
    "outputs = {\n",
    "    \"iter\": [],\n",
    "    \"k\": [],\n",
    "    \"proportion\": [],\n",
    "    \"test_idx\": [],\n",
    "    \"task\": [],\n",
    "    \"output\": []\n",
    "}\n",
    "\n",
    "llm = Llama3_1_Instruct()\n",
    "\n",
    "num_iters = 5\n",
    "num_tasks = 4\n",
    "for i in range(num_iters):\n",
    "    \n",
    "\n",
    "    for t in range(num_tasks):\n",
    "        print(f\"Iter {i} Task {t}\")\n",
    "\n",
    "        test_dl = tests_dl[t]\n",
    "        test_idxs = next(iter(test_dl))\n",
    "\n",
    "        for test_idx in test_idxs[0]:\n",
    "\n",
    "            test_idx = test_idx[0].item()\n",
    "\n",
    "            for k in [1,2,4]:\n",
    "                for setting in proportion_settings[k]:\n",
    "\n",
    "                    train_dl = collection_loaders_1[\"task_{}\".format(t)]\n",
    "                    collections_idxs = next(iter(train_dl))\n",
    "\n",
    "                    for c in collections_idx[0]:\n",
    "                        c = c.item()\n",
    "\n",
    "                        train_idxs = collections[c].select(\"indices\").to_numpy()[0][0].tolist()\n",
    "\n",
    "                        prompt = set_instruction_prompt(default, train, train_idxs, test_idx, test, instructions, setting[0][0], setting[0][1], tasks[t].item())\n",
    "                        output = llm.run(prompt)\n",
    "\n",
    "                        outputs[\"iter\"].append(i)\n",
    "                        outputs[\"proportion\"].append(setting[1])\n",
    "                        outputs[\"k\"] = k\n",
    "                        outputs[\"test_idx\"].append(test_idx)\n",
    "                        outputs[\"task\"].append(tasks[t].item())\n",
    "                        outputs[\"output\"].append(output)\n",
    "                        \n",
    "                        results = pl.DataFrame(outputs).write_ipc(f\"{dataset_path}/results/results_llama_1_4.feather\")\n",
    "\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
