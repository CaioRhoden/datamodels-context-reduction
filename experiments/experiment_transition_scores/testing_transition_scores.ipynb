{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Approaches to get output log probs sequences\n",
    "\n",
    "The goal of this notebook is to study and explore ways to use the log prob output of a language models in comparsion with a target sequence to estimate the importance of a given context\n",
    "\n",
    "\n",
    "## What are we using here?\n",
    "\n",
    "* Model: GPT2: a light baseline to allows us to quick handle the outputs and generatios\n",
    "* Methods: generate() - given a sequence of inputs it returns the scores and logits (if specified to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Visualization of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Today is a nice day\", return_tensors=\"pt\").input_ids\n",
    "generated_outputs = gpt2.generate(input_ids, \n",
    "                                  output_scores=True, \n",
    "                                  length_penalty=0, \n",
    "                                  output_logits=True,\n",
    "                                  return_dict_in_generate=True,\n",
    "                                  max_new_tokens=3,\n",
    "                                  num_beams=2,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.utils.GenerateBeamDecoderOnlyOutput"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'sequences_scores', 'scores', 'logits', 'beam_indices', 'past_key_values'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8888,  318,  257, 3621, 1110,   11,  475,  314]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.8570,  -6.2563, -13.6480,  ..., -20.5584, -17.1551,  -9.5375],\n",
       "        [ -3.8570,  -6.2563, -13.6480,  ..., -20.5584, -17.1550,  -9.5375]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs.scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -88.5250,  -90.9243,  -98.3160,  ..., -105.2264, -101.8231,\n",
       "          -94.2055],\n",
       "        [ -88.5250,  -90.9243,  -98.3160,  ..., -105.2264, -101.8231,\n",
       "          -94.2055]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is a nice day, but I\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(generated_outputs.sequences[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Increment Context Approach\n",
    "\n",
    "In this approach the log_prob is calculated for each expected target token where they are generated one by one and them added to the context in order to be done the next prediction\n",
    "\n",
    "Link: https://discuss.huggingface.co/t/compute-log-probabilities-of-any-sequence-provided/11710/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_context(input_token, target_output, model, tokenizer):\n",
    "\n",
    "    log_sum = 0\n",
    "    input_tokens_updated = input_tokens.clone().to(torch.int64).to(device)\n",
    "\n",
    "    for i in range(len(target_output)):\n",
    "        # Predict with the given model\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_tokens_updated, max_new_tokens=1, output_logits=True, return_dict_in_generate=True, pad_token_id=50256)\n",
    "            logit_predictions = outputs.logits[0]\n",
    "    \n",
    "        # Extract the log probability of the output token\n",
    "        token = tokenizer.decode(target_output[i])\n",
    "        log_probs = torch.nn.functional.log_softmax(logit_predictions, dim=-1)\n",
    "        out_token_logit = logit_predictions[0, target_output[i]]\n",
    "        out_token_log_prob = log_probs[0, target_output[i]]\n",
    "        log_sum += out_token_log_prob\n",
    "        print(f\"Token: {token}, logit: {out_token_logit}, log prob: {out_token_log_prob}\")\n",
    "\n",
    "\n",
    "        predicted_token = tokenizer.decode(outputs.sequences[0][-1])\n",
    "        predicted_logit = logit_predictions[0, outputs.sequences[0][-1]]\n",
    "        predicted_log_prob = log_probs[0, outputs.sequences[0][-1]]\n",
    "        print(f\"Predicted Token: {predicted_token}, logit: {predicted_logit}, log prob: {predicted_log_prob}\")\n",
    "    \n",
    "        # Incrementally add an output token to the current sequence\n",
    "        input_tokens_updated = torch.cat([input_tokens_updated, target_output[i].reshape(1, 1)], dim=1)\n",
    "        print([tokenizer.decode(token) for token in input_tokens_updated])\n",
    "        print(\"============\")\n",
    "        print()\n",
    "    print(f\"Total Log Sum Probability: {log_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"Today is a nice day\", add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "target_output = tokenizer.encode(\", and tomorrow it will be as well\", add_special_tokens=False, return_tensors=\"pt\")[0].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8888,  318,  257, 3621, 1110]], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  11,  290, 9439,  340,  481,  307,  355,  880], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ,, logit: -86.54722595214844, log prob: -1.8792250156402588\n",
      "Predicted Token:  for, logit: -86.18283081054688, log prob: -1.5148298740386963\n",
      "['Today is a nice day,']\n",
      "============\n",
      "\n",
      "Token:  and, logit: -98.12623596191406, log prob: -2.3308000564575195\n",
      "Predicted Token:  but, logit: -97.56433868408203, log prob: -1.7689028978347778\n",
      "['Today is a nice day, and']\n",
      "============\n",
      "\n",
      "Token:  tomorrow, logit: -120.85678100585938, log prob: -5.777646064758301\n",
      "Predicted Token:  I, logit: -116.57794189453125, log prob: -1.4988069534301758\n",
      "['Today is a nice day, and tomorrow']\n",
      "============\n",
      "\n",
      "Token:  it, logit: -85.92831420898438, log prob: -3.039114236831665\n",
      "Predicted Token:  is, logit: -83.59170532226562, log prob: -0.7025054097175598\n",
      "['Today is a nice day, and tomorrow it']\n",
      "============\n",
      "\n",
      "Token:  will, logit: -71.38861083984375, log prob: -1.4177868366241455\n",
      "Predicted Token: 's, logit: -70.89265441894531, log prob: -0.921830415725708\n",
      "['Today is a nice day, and tomorrow it will']\n",
      "============\n",
      "\n",
      "Token:  be, logit: -89.09539794921875, log prob: -0.24227555096149445\n",
      "Predicted Token:  be, logit: -89.09539794921875, log prob: -0.24227555096149445\n",
      "['Today is a nice day, and tomorrow it will be']\n",
      "============\n",
      "\n",
      "Token:  as, logit: -109.92655944824219, log prob: -5.756874084472656\n",
      "Predicted Token:  a, logit: -105.66957092285156, log prob: -1.4998857975006104\n",
      "['Today is a nice day, and tomorrow it will be as']\n",
      "============\n",
      "\n",
      "Token:  well, logit: -95.97348022460938, log prob: -4.663259029388428\n",
      "Predicted Token:  good, logit: -92.76578521728516, log prob: -1.455564022064209\n",
      "['Today is a nice day, and tomorrow it will be as well']\n",
      "============\n",
      "\n",
      "Total Log Sum Probability: -25.10698127746582\n"
     ]
    }
   ],
   "source": [
    "increment_context(input_tokens, target_output, gpt2.to(device), tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datamodes)",
   "language": "python",
   "name": "datamodels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
