{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Runs - Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "## LLM\n",
    "from src.taco_evaluator import compute, compute_1_pass_by_test\n",
    "from datasets import load_from_disk\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH  = \"../../../data/TACO/processed\"\n",
    "train = pl.read_ipc(f\"{PATH}/train.feather\")\n",
    "train_solutions = pl.read_ipc(f\"{PATH}/train_solutions.feather\")\n",
    "train_dict = load_from_disk(\"../../../data/TACO/train.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generation(generations: list, id: int, path: str):\n",
    "    \n",
    "    gens = []\n",
    "    for i in range(len(generations)):\n",
    "\n",
    "        code_blocks = re.findall(r'```python(.*?)```', generations[i][\"generated_text\"], re.DOTALL)\n",
    "        extracted_code = \"\\n\".join([block.strip() for block in code_blocks])\n",
    "        gens.append(extracted_code)\n",
    "    \n",
    "    results = {\n",
    "        \"task_id\": int(id),\n",
    "        \"output\": gens\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_jsons(repo:str) -> List[Dict[str, Any]]:\n",
    "    jsons = []\n",
    "    for file in os.listdir(repo):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(f\"{repo}/{file}\", \"r\") as f:\n",
    "                jsons.append(json.load(f))\n",
    "    return jsons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EASY Examples Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_results = join_jsons(\"../outputs_easy_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = []\n",
    "for r in easy_results:\n",
    "    parsed.append(parse_generation(r[\"generations\"], r[\"id\"], \"small_experiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = [r[\"id\"] for r in easy_results]\n",
    "partial_train = []\n",
    "for i in selected_ids:\n",
    "    partial_train.append(train_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(parsed, open(\"parsed_small_experiment.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "\n",
      "00000\n",
      "00000\n",
      "00000\n",
      "00000\n",
      "00000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparsed_small_experiment.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/compute_metrics.py:172\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(generation_file, taco, k_pass, debug, file, return_dict)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(generation_file: \u001b[38;5;28mstr\u001b[39m, taco, k_pass:\u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m], debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaco_metrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Initialize evaluation dataset with the same setup with generation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# difficulties = ['ALL']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# taco = load_dataset('BAAI/TACO', split='test', difficulties=difficulties)\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# taco = load_dataset('BAAI/TACO', split='test', skills=skills)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     generations \u001b[38;5;241m=\u001b[39m load_generation(generation_file)\n\u001b[0;32m--> 172\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_generations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaco\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# You can use evaluate_generations_parallel to parallel executing multiple outputs for each problem\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# results = evaluate_generations_parallel(generations, taco)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m compute_metrics(results, k_list\u001b[38;5;241m=\u001b[39mk_pass)\n",
      "File \u001b[0;32m/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/compute_metrics.py:54\u001b[0m, in \u001b[0;36mevaluate_generations\u001b[0;34m(generations, samples, idx, debug)\u001b[0m\n\u001b[1;32m     52\u001b[0m curr_res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     curr_res \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_correctness\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSuccessful compilation of task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/compute_metrics.py:19\u001b[0m, in \u001b[0;36mcheck_correctness\u001b[0;34m(sample, generation, timeout, debug)\u001b[0m\n\u001b[1;32m     16\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(run_test(sample, test\u001b[38;5;241m=\u001b[39mgeneration, debug\u001b[38;5;241m=\u001b[39mdebug))\n\u001b[1;32m     18\u001b[0m manager \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mManager()\n\u001b[0;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m p \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mProcess(target\u001b[38;5;241m=\u001b[39m_temp_run, args\u001b[38;5;241m=\u001b[39m(sample, generation, debug, result))\n\u001b[1;32m     21\u001b[0m p\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/managers.py:727\u001b[0m, in \u001b[0;36mBaseManager.register.<locals>.temp\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtemp\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    726\u001b[0m     util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequesting creation of a shared \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m'\u001b[39m, typeid)\n\u001b[0;32m--> 727\u001b[0m     token, exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypeid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m     proxy \u001b[38;5;241m=\u001b[39m proxytype(\n\u001b[1;32m    729\u001b[0m         token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serializer, manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    730\u001b[0m         authkey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authkey, exposed\u001b[38;5;241m=\u001b[39mexp\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    732\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Client(token\u001b[38;5;241m.\u001b[39maddress, authkey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authkey)\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/managers.py:607\u001b[0m, in \u001b[0;36mBaseManager._create\u001b[0;34m(self, typeid, *args, **kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03mCreate a new shared object; return the token and exposed tuple\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m State\u001b[38;5;241m.\u001b[39mSTARTED, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver not yet started\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 607\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Client\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_authkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mid\u001b[39m, exposed \u001b[38;5;241m=\u001b[39m dispatch(conn, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m'\u001b[39m, (typeid,)\u001b[38;5;241m+\u001b[39margs, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/connection.py:526\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     answer_challenge(c, authkey)\n\u001b[0;32m--> 526\u001b[0m     \u001b[43mdeliver_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/connection.py:755\u001b[0m, in \u001b[0;36mdeliver_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthkey must be bytes, not \u001b[39m\u001b[38;5;132;01m{0!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(authkey)))\n\u001b[1;32m    754\u001b[0m message \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39murandom(MESSAGE_LENGTH)\n\u001b[0;32m--> 755\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHALLENGE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m digest \u001b[38;5;241m=\u001b[39m hmac\u001b[38;5;241m.\u001b[39mnew(authkey, message, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmd5\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdigest()\n\u001b[1;32m    757\u001b[0m response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mrecv_bytes(\u001b[38;5;241m256\u001b[39m)        \u001b[38;5;66;03m# reject large message\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/connection.py:200\u001b[0m, in \u001b[0;36m_ConnectionBase.send_bytes\u001b[0;34m(self, buf, offset, size)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m offset \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m>\u001b[39m n:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffer length < offset + size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m:\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/connection.py:427\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/connection.py:384\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    382\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     n \u001b[38;5;241m=\u001b[39m write(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle, buf)\n\u001b[1;32m    385\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compute(\"parsed_small_experiment.json\", partial_train, [1,10,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "\n",
      "00000\n",
      "00000\n",
      "00000\n",
      "00000\n",
      "00000\n",
      "\n",
      "26\n",
      "52\n",
      "2\n",
      "52\n",
      "33554432\n",
      "78\n",
      "4\n",
      "4\n",
      "26\n",
      "50\n",
      "5\n",
      "6\n",
      "10\n",
      "416\n",
      "6\n",
      "1\n",
      "2\n",
      "26\n",
      "2\n",
      "3099044504245996706400\n",
      "8\n",
      "16\n",
      "308\n",
      "67108864\n",
      "10715086071862673209484250490600018105614048117055336074437503883703510511249361224931983788156958581275946729175531468251871452856923140435984577574698574803934567774824230985421074605062371141877954182153046474983581941267398767559165543946077062914571196477686542167660429831652624386837205668069376\n",
      "78\n",
      "6\n",
      "11\n",
      "26\n",
      "1000\n",
      "4\n",
      "4\n",
      "0\n",
      "alarm went off\n",
      "48\n",
      "48\n",
      "2000\n",
      "13\n",
      "1\n",
      "5\n",
      "13\n",
      "1\n",
      "500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_1_pass_by_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparsed_small_experiment.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaco_easy_1_pass.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/compute_1_pass_by_test.py:148\u001b[0m, in \u001b[0;36mcompute_1_pass_by_test\u001b[0;34m(generation_file, taco, debug, file, return_dict, return_results)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_1_pass_by_test\u001b[39m(generation_file: \u001b[38;5;28mstr\u001b[39m, taco, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaco_1_pass_metrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, return_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Initialize evaluation dataset with the same setup with generation\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# difficulties = ['ALL']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# taco = load_dataset('BAAI/TACO', split='test', difficulties=difficulties)\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# taco = load_dataset('BAAI/TACO', split='test', skills=skills)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     generations \u001b[38;5;241m=\u001b[39m load_generation(generation_file)\n\u001b[0;32m--> 148\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_generations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaco\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# You can use evaluate_generations_parallel to parallel executing multiple outputs for each problem\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# results = evaluate_generations_parallel(generations, taco)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m calculate_1_pass(results)\n",
      "File \u001b[0;32m/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/compute_1_pass_by_test.py:55\u001b[0m, in \u001b[0;36mevaluate_generations\u001b[0;34m(generations, samples, idx, debug)\u001b[0m\n\u001b[1;32m     53\u001b[0m curr_res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     curr_res \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_correctness\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSuccessful compilation of task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/compute_1_pass_by_test.py:23\u001b[0m, in \u001b[0;36mcheck_correctness\u001b[0;34m(sample, generation, timeout, debug)\u001b[0m\n\u001b[1;32m     21\u001b[0m p \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mProcess(target\u001b[38;5;241m=\u001b[39m_temp_run, args\u001b[38;5;241m=\u001b[39m(sample, generation, debug, result))\n\u001b[1;32m     22\u001b[0m p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m     25\u001b[0m     p\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, flag)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-153:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/compute_1_pass_by_test.py\", line 17, in _temp_run\n",
      "    result.append(run_test(sample, test=generation, debug=debug))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/metrics/testing_util.py\", line 87, in run_test\n",
      "    detail_results = execute_std_code(exec_code, inputs_list, outputs_list, timeout=TIMEOUT, early_stop=False, debug=debug)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/work/caio.rhoden/datamodels-context-reduction/src/taco_evaluator/metrics/testing_util.py\", line 305, in execute_std_code\n",
      "    result = subprocess.run(['python', temp_program_path], input=inputs, text=True, capture_output=True, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/subprocess.py\", line 550, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/subprocess.py\", line 1209, in communicate\n",
      "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/subprocess.py\", line 2115, in _communicate\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "compute_1_pass_by_test(\"parsed_small_experiment.json\", partial_train, file=\"taco_easy_1_pass.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
