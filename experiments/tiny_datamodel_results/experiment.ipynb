{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamodels Retriever Experiment\n",
    "\n",
    "This document has the goal to show the implementation of a context retriever using Datamodels and its comparison against classical approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Run Classical Approaches\n",
    "\n",
    "First it's needed to run the comparison subject, to do so we will use the script present in this folder, it's just necessary to run\n",
    "\n",
    "```\n",
    "python run_classical_retriever.py\n",
    "```\n",
    "\n",
    "This will run the retriever for each sample from the test dataset, saving the data at every 50 samples as checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split Data for Datamodeling\n",
    "\n",
    "Here we will be spliting the data to achieve a dev dataset containing a representative numbe of samples to each subtask\n",
    "The \"k\" used here is 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.utils import split_dev_set, subset_df\n",
    "from src.retriever import DatamodelsRetriever\n",
    "from src.datamodels_pipeline import DatamodelPipeline, DatamodelConfig, MemMapConfig\n",
    "from src.evaluator import Rouge_L_evaluator\n",
    "from src.llms import Llama3_1\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# Limit available GPUs to GPU 0 and 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(\"../../data/instruction-induction-data/processed/induce_tasks_examples.csv\")\n",
    "# train_subset = subset_df(train, 200, \"task\")\n",
    "# train_subset.to_csv(\"../../data/instruction-induction-data/processed/train.csv\")\n",
    "\n",
    "# split_dev_set(\n",
    "#     path=\"../../data/instruction-induction-data/processed/train.csv\",\n",
    "#     saving_path=\"../../data/instruction-induction-data/datamodels\",    \n",
    "#     k_samples=15,\n",
    "#     task_column=\"task\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split the Collections to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### First time create collection #####\n",
    "# retriever = DatamodelsRetriever(k=8)\n",
    "# retriever.create_collections_index(\n",
    "#     \"../../data/instruction-induction-data/datamodels_15_10_2024/train_set.csv\",\n",
    "#     \"../../data/instruction-induction-data/datamodels_15_10_2024\",\n",
    "#     n_samples=500,\n",
    "#     test_per=0.2,\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the best vegatable for salad?\\nI love eating salad. I eat it every day. I have a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama = Llama3_1()\n",
    "llama.run(\"What is the best vegatable for salad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Datamodels config for experiment #########\n",
    "config = DatamodelConfig(\n",
    "    k = 8,\n",
    "    num_models= 315,\n",
    "    datamodels_path = \"../../data/instruction-induction-data/datamodels_15_10_2024\",\n",
    "    train_collections_idx = None,\n",
    "    test_collections_idx = None,\n",
    "    test_set = None,\n",
    "    train_set = None,\n",
    "    instructions= None,\n",
    "    llm = llama,\n",
    "    evaluator=Rouge_L_evaluator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train collection index\n",
      "Loaded train set\n",
      "Loaded test set\n"
     ]
    }
   ],
   "source": [
    "datamodel = DatamodelPipeline(config)\n",
    "datamodel.set_collections_index()\n",
    "datamodel.set_dataframes()\n",
    "datamodel.set_instructions_from_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"../../data/instruction-induction-data/datamodels_15_10_2024/X_train.npy\", allow_pickle=True)\n",
    "y = np.load(\"../../data/instruction-induction-data/datamodels_15_10_2024/Y_train.npy\", allow_pickle=True)\n",
    "# competed = np.load(\"../../data/instruction-induction-data/datamodels_15_10_2024/_completed.npy\", allow_pickle=True)\n",
    "# print(x.shape, y.shape, competed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315, 401)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process Process-481:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 113, in worker_job_indexed_dataset\n",
      "    handle_sample(sample, dest_ix, field_names, metadata, allocator, fields)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 51, in handle_sample\n",
      "    field.encode(destination, field_value, allocator.malloc)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/fields/ndarray.py\", line 99, in encode\n",
      "    data_region[:] = field.reshape(-1).view('<u1')\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Process Process-482:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process Process-483:\n",
      "TypeError: view() received an invalid combination of arguments - got (str), but expected one of:\n",
      " * (torch.dtype dtype)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      " * (tuple of ints size)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 113, in worker_job_indexed_dataset\n",
      "    handle_sample(sample, dest_ix, field_names, metadata, allocator, fields)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 51, in handle_sample\n",
      "    field.encode(destination, field_value, allocator.malloc)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/fields/ndarray.py\", line 99, in encode\n",
      "    data_region[:] = field.reshape(-1).view('<u1')\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: view() received an invalid combination of arguments - got (str), but expected one of:\n",
      " * (torch.dtype dtype)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      " * (tuple of ints size)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      "\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 113, in worker_job_indexed_dataset\n",
      "    handle_sample(sample, dest_ix, field_names, metadata, allocator, fields)\n",
      "Process Process-485:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 51, in handle_sample\n",
      "    field.encode(destination, field_value, allocator.malloc)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/fields/ndarray.py\", line 99, in encode\n",
      "    data_region[:] = field.reshape(-1).view('<u1')\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "TypeError: view() received an invalid combination of arguments - got (str), but expected one of:\n",
      " * (torch.dtype dtype)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      " * (tuple of ints size)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      "\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 113, in worker_job_indexed_dataset\n",
      "    handle_sample(sample, dest_ix, field_names, metadata, allocator, fields)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/writer.py\", line 51, in handle_sample\n",
      "    field.encode(destination, field_value, allocator.malloc)\n",
      "TypeError: view() received an invalid combination of arguments - got (str), but expected one of:\n",
      " * (torch.dtype dtype)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      " * (tuple of ints size)\n",
      "      didn't match because some of the arguments have invalid types: (!str!)\n",
      "\n",
      "  File \"/home/caio.rhoden/miniconda3/envs/datamodels/lib/python3.11/site-packages/ffcv/fields/ndarray.py\", line 99, in encode\n",
      "    data_region[:] = field.reshape(-1).view('<u1')\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import FloatField, NDArrayField\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "writer = DatasetWriter(\"../../data/instruction-induction-data/datamodels_15_10_2024/ffcv_dataset\", {\n",
    "    # Tune options to optimize dataset size, throughput at train-time\n",
    "    'mask':NDArrayField(\n",
    "        dtype=np.dtype('bool'),\n",
    "        shape=(401,3885)\n",
    "    ),\n",
    "    'mask':NDArrayField(\n",
    "        dtype=np.dtype('float32'),\n",
    "        shape=(401,)\n",
    "    ),\n",
    "})\n",
    "\n",
    "# Write dataset\n",
    "X_tensor = torch.tensor(x, dtype=torch.bool)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)  # use long for classification labels\n",
    "\n",
    "# Create a TensorDataset\n",
    "my_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "writer.from_indexed_dataset(my_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([401, 3885])\n",
      "torch.Size([401])\n"
     ]
    }
   ],
   "source": [
    "print(my_dataset[0][0].shape)\n",
    "print(my_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, weights, bias):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weights) + self.bias\n",
    "\n",
    "weights = torch.randn(3885, requires_grad=True)  # Randomly initialized weights\n",
    "bias = torch.randn(1, requires_grad=True)  \n",
    "model = LinearRegressor(weights, bias)\n",
    "\n",
    "# Step 5: Define loss function and optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD([weights, bias], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 4.5410\n",
      "Epoch [2/20], Loss: 4.1242\n",
      "Epoch [3/20], Loss: 3.7638\n",
      "Epoch [4/20], Loss: 3.4479\n",
      "Epoch [5/20], Loss: 3.1689\n",
      "Epoch [6/20], Loss: 2.9209\n",
      "Epoch [7/20], Loss: 2.6994\n",
      "Epoch [8/20], Loss: 2.5006\n",
      "Epoch [9/20], Loss: 2.3214\n",
      "Epoch [10/20], Loss: 2.1593\n",
      "Epoch [11/20], Loss: 2.0123\n",
      "Epoch [12/20], Loss: 1.8784\n",
      "Epoch [13/20], Loss: 1.7562\n",
      "Epoch [14/20], Loss: 1.6445\n",
      "Epoch [15/20], Loss: 1.5420\n",
      "Epoch [16/20], Loss: 1.4478\n",
      "Epoch [17/20], Loss: 1.3611\n",
      "Epoch [18/20], Loss: 1.2811\n",
      "Epoch [19/20], Loss: 1.2073\n",
      "Epoch [20/20], Loss: 1.1390\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "\n",
    "    for idx in range(len(my_dataset)):\n",
    "        # Apply the mask to the weights\n",
    "        masked_weights = weights * my_dataset[idx][0]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(masked_weights.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, my_dataset[idx][1].unsqueeze(0))  # Add batch dimension to target\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{20}], Loss: {total_loss / 401:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1404,  0.7905, -0.2211,  ..., -1.1051, -0.2606,  1.7707],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection id: 0\n",
      "Checkpoint 0 saved\n"
     ]
    }
   ],
   "source": [
    "# datamodel.create_pre_collection(start_idx=0, end_idx=1)\n",
    "\n",
    "# template = \"\"\"\n",
    "#             Fill the expected Output according to the instruction\n",
    "#             Intruction: {instruction}\n",
    "\n",
    "#             Examples:\n",
    "#             {context}\n",
    "\n",
    "#             User Input:\n",
    "#             {input}\n",
    "\n",
    "#             Model Output:\n",
    "#         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection_idx</th>\n",
       "      <th>test_idx</th>\n",
       "      <th>input</th>\n",
       "      <th>predicted_output</th>\n",
       "      <th>true_output</th>\n",
       "      <th>optinal_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The manager was mentioned by the judge.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The presidents were encouraged by the professors.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The secretary was recommended by the banker.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The presidents were thanked by the secretaries.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The doctor was recognized by the bankers.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   collection_idx  test_idx  \\\n",
       "0               0         0   \n",
       "1               0         1   \n",
       "2               0         2   \n",
       "3               0         3   \n",
       "4               0         4   \n",
       "\n",
       "                                               input  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    predicted_output  \\\n",
       "0  \\n            Fill the expected Output accordi...   \n",
       "1  \\n            Fill the expected Output accordi...   \n",
       "2  \\n            Fill the expected Output accordi...   \n",
       "3  \\n            Fill the expected Output accordi...   \n",
       "4  \\n            Fill the expected Output accordi...   \n",
       "\n",
       "                                         true_output optinal_output  \n",
       "0            The manager was mentioned by the judge.            NaN  \n",
       "1  The presidents were encouraged by the professors.            NaN  \n",
       "2       The secretary was recommended by the banker.            NaN  \n",
       "3    The presidents were thanked by the secretaries.            NaN  \n",
       "4          The doctor was recognized by the bankers.            NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle( \"../../data/instruction-induction-data/datamodels_15_10_2024/pre_collections/15-10-2024/pre_collection_0.pickle\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_output(text):\n",
    "    # Split the string and strip leading/trailing spaces and newlines\n",
    "    return text.split(\" Model Output:\\n \", 1)[-1].strip()\n",
    "\n",
    "df = pd.read_pickle( \"../../data/instruction-induction-data/datamodels_15_10_2024/pre_collections/15-10-2024/pre_collection_0.pickle\")\n",
    "\n",
    "\n",
    "result = Rouge_L_evaluator().evaluate(df[\"true_output\"].to_numpy(),  df[\"predicted_output\"].apply(extract_output).to_numpy(), datamodel.llm.tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.71428571, 1.        , 1.        , 1.        ,\n",
       "       0.77777778, 1.        , 1.        , 0.77777778, 1.        ,\n",
       "       1.        , 0.77777778, 1.        , 1.        , 1.        ,\n",
       "       0.28571429, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.66666667, 1.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4       , 0.        ,\n",
       "       0.33333333, 0.        , 1.        , 0.28571429, 0.        ,\n",
       "       0.5       , 0.        , 0.        , 0.        , 0.5       ,\n",
       "       0.        , 0.28571429, 0.        , 1.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.4       ,\n",
       "       0.        , 0.        , 0.18181818, 0.        , 1.        ,\n",
       "       0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.66666667, 0.28571429,\n",
       "       0.        , 0.4       , 1.        , 0.33333333, 0.        ,\n",
       "       0.33333333, 0.        , 0.66666667, 0.28571429, 0.33333333,\n",
       "       0.28571429, 0.        , 0.15384615, 0.        , 0.66666667,\n",
       "       0.        , 0.4       , 0.        , 0.        , 0.44444444,\n",
       "       0.        , 0.90909091, 0.88888889, 0.18181818, 0.18181818,\n",
       "       0.85714286, 0.83333333, 0.66666667, 0.42857143, 0.95652174,\n",
       "       0.57142857, 0.75      , 0.5       , 0.93333333, 0.53333333,\n",
       "       0.70588235, 0.84210526, 0.55555556, 0.72727273, 0.47058824,\n",
       "       0.92307692, 0.90909091, 1.        , 1.        , 0.92307692,\n",
       "       0.88888889, 0.66666667, 0.77777778, 0.88888889, 0.8       ,\n",
       "       0.44444444, 0.92307692, 0.90909091, 0.66666667, 0.75      ,\n",
       "       0.16666667, 0.4       , 0.16666667, 0.16666667, 0.25      ,\n",
       "       0.        , 0.5       , 0.33333333, 0.18181818, 0.125     ,\n",
       "       0.        , 0.18181818, 0.42857143, 0.16666667, 0.28571429,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.33333333, 1.        , 0.        , 0.22222222, 0.        ,\n",
       "       0.6       , 0.        , 0.33333333, 0.22222222, 0.22222222,\n",
       "       0.4       , 0.        , 0.2       , 0.4       , 0.        ,\n",
       "       0.        , 0.28571429, 0.        , 0.        , 0.33333333,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.28571429, 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "       1.        , 0.28571429, 1.        , 0.5       , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "       0.        , 0.22222222, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.2       , 0.25      , 0.4       , 0.        , 0.25      ,\n",
       "       0.        , 0.44444444, 0.        , 0.44444444, 0.4       ,\n",
       "       0.        , 0.5       , 0.75      , 0.5       , 0.        ,\n",
       "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.25      , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.125     , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.15384615, 0.        , 0.2       ,\n",
       "       0.        , 0.11764706, 0.2       , 0.        , 0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection_idx</th>\n",
       "      <th>test_idx</th>\n",
       "      <th>input</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   collection_idx  test_idx  \\\n",
       "0               0         0   \n",
       "1               0         1   \n",
       "2               0         2   \n",
       "3               0         3   \n",
       "4               0         4   \n",
       "\n",
       "                                               input  evaluation  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    1.000000  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    0.714286  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    1.000000  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    0.823529  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    0.777778  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collection = pd.read_pickle( \"../../data/instruction-induction-data/datamodels_15_10_2024/collections/15-10-2024/pre_collection_10.pickle\")\n",
    "df_collection.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>possible_outputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The lawyer avoided the secretaries.</td>\n",
       "      <td>The secretaries were avoided by the lawyer.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The judges supported the scientists.</td>\n",
       "      <td>The scientists were supported by the judges.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The secretary recommended the professor.</td>\n",
       "      <td>The professor was recommended by the secretary.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The artists recommended the secretary.</td>\n",
       "      <td>The secretary was recommended by the artists.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The secretary avoided the senator.</td>\n",
       "      <td>The senator was avoided by the secretary.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                task                                     input  \\\n",
       "0  active_to_passive       The lawyer avoided the secretaries.   \n",
       "1  active_to_passive      The judges supported the scientists.   \n",
       "2  active_to_passive  The secretary recommended the professor.   \n",
       "3  active_to_passive    The artists recommended the secretary.   \n",
       "4  active_to_passive        The secretary avoided the senator.   \n",
       "\n",
       "                                            output possible_outputs  \n",
       "0      The secretaries were avoided by the lawyer.              NaN  \n",
       "1     The scientists were supported by the judges.              NaN  \n",
       "2  The professor was recommended by the secretary.              NaN  \n",
       "3    The secretary was recommended by the artists.              NaN  \n",
       "4        The senator was avoided by the secretary.              NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind_df = pd.read_csv(\"../../data/instruction-induction-data/datamodels_15_10_2024/train_set.csv\")\n",
    "traind_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>possible_outputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The judge mentioned the manager.</td>\n",
       "      <td>The manager was mentioned by the judge.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The professors encouraged the presidents.</td>\n",
       "      <td>The presidents were encouraged by the professors.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The banker recommended the secretary.</td>\n",
       "      <td>The secretary was recommended by the banker.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The secretaries thanked the presidents.</td>\n",
       "      <td>The presidents were thanked by the secretaries.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>The bankers recognized the doctor.</td>\n",
       "      <td>The doctor was recognized by the bankers.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                task                                      input  \\\n",
       "0  active_to_passive           The judge mentioned the manager.   \n",
       "1  active_to_passive  The professors encouraged the presidents.   \n",
       "2  active_to_passive      The banker recommended the secretary.   \n",
       "3  active_to_passive    The secretaries thanked the presidents.   \n",
       "4  active_to_passive         The bankers recognized the doctor.   \n",
       "\n",
       "                                              output possible_outputs  \n",
       "0            The manager was mentioned by the judge.              NaN  \n",
       "1  The presidents were encouraged by the professors.              NaN  \n",
       "2       The secretary was recommended by the banker.              NaN  \n",
       "3    The presidents were thanked by the secretaries.              NaN  \n",
       "4          The doctor was recognized by the bankers.              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = pd.read_csv(\"../../data/instruction-induction-data/datamodels_15_10_2024/dev_set.csv\")\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection_idx</th>\n",
       "      <th>test_idx</th>\n",
       "      <th>input</th>\n",
       "      <th>predicted_output</th>\n",
       "      <th>true_output</th>\n",
       "      <th>optinal_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The manager was mentioned by the judge.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The presidents were encouraged by the professors.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The secretary was recommended by the banker.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The presidents were thanked by the secretaries.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>\\n            Fill the expected Output accordi...</td>\n",
       "      <td>The doctor was recognized by the bankers.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   collection_idx  test_idx  \\\n",
       "0               0         0   \n",
       "1               0         1   \n",
       "2               0         2   \n",
       "3               0         3   \n",
       "4               0         4   \n",
       "\n",
       "                                               input  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    predicted_output  \\\n",
       "0  \\n            Fill the expected Output accordi...   \n",
       "1  \\n            Fill the expected Output accordi...   \n",
       "2  \\n            Fill the expected Output accordi...   \n",
       "3  \\n            Fill the expected Output accordi...   \n",
       "4  \\n            Fill the expected Output accordi...   \n",
       "\n",
       "                                         true_output optinal_output  \n",
       "0            The manager was mentioned by the judge.            NaN  \n",
       "1  The presidents were encouraged by the professors.            NaN  \n",
       "2       The secretary was recommended by the banker.            NaN  \n",
       "3    The presidents were thanked by the secretaries.            NaN  \n",
       "4          The doctor was recognized by the bankers.            NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_collection = pd.read_pickle( \"../../data/instruction-induction-data/datamodels_15_10_2024/pre_collections/15-10-2024/pre_collection_10.pickle\")\n",
    "pre_collection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection_idx</th>\n",
       "      <th>test_idx</th>\n",
       "      <th>input</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   collection_idx  test_idx  \\\n",
       "0               0         0   \n",
       "1               0         1   \n",
       "2               0         2   \n",
       "3               0         3   \n",
       "4               0         4   \n",
       "\n",
       "                                               input  evaluation  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    1.000000  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    0.714286  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    1.000000  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    0.823529  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    0.777778  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = pd.read_pickle( \"../../data/instruction-induction-data/datamodels_15_10_2024/collections/15-10-2024/pre_collection_10.pickle\")\n",
    "collection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Sample Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59303/1521282305.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample[\"evaluation\"] = collection.copy().loc[0][\"evaluation\"]\n",
      "/tmp/ipykernel_59303/1521282305.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample[\"evaluation\"] = collection.copy().loc[0][\"evaluation\"]\n",
      "/tmp/ipykernel_59303/1521282305.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample[\"task\"] = dev_df.loc[sample[\"test_idx\"]][\"task\"]\n",
      "/tmp/ipykernel_59303/1521282305.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample[\"task\"] = dev_df.loc[sample[\"test_idx\"]][\"task\"]\n"
     ]
    }
   ],
   "source": [
    "### Anlyse a sing sample\n",
    "sample = pre_collection.loc[0]\n",
    "sample[\"evaluation\"] = collection.copy().loc[0][\"evaluation\"]\n",
    "sample[\"task\"] = dev_df.loc[sample[\"test_idx\"]][\"task\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Fill the expected Output according to the instruction\n",
      "            Intruction: Write the input sentence in passive form.\n",
      "\n",
      "            Examples:\n",
      "            Input: 5040 \n",
      "Output: five thousand and forty\n",
      "Input: 1 64 \n",
      "Output: 65\n",
      "Input: grenade \n",
      "Output: r\n",
      "Input: Sentence 1: A school bus is driving uphill on a rural road. Sentence 2: A race care driving along a dirt road. \n",
      "Output: 1 - probably not\n",
      "Input: souvenir \n",
      "Output: near\n",
      "Input: The student recognized the professors. \n",
      "Output: The professors were recognized by the student.\n",
      "Input: Sentence 1: White House in damage control over Obama Supreme Court remarks Sentence 2: Fact check: Obama's Supreme Court remarks \n",
      "Output: 4 - almost perfectly\n",
      "Input: camouflage \n",
      "Output: c\n",
      "\n",
      "\n",
      "            User Input:\n",
      "            The judge mentioned the manager.\n",
      "\n",
      "            Model Output:\n",
      "         The manager was mentioned by the judge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"predicted_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The manager was mentioned by the judge.\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"true_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"evaluation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results by Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pre_collection.copy()\n",
    "samples[\"evaluation\"] = collection[\"evaluation\"]\n",
    "samples[\"task\"] = samples[\"test_idx\"].apply(lambda idx: dev_df.loc[idx, 'task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task\n",
       "active_to_passive          165\n",
       "antonyms                   165\n",
       "diff                       165\n",
       "first_word_letter          165\n",
       "larger_animal              165\n",
       "letters_list               165\n",
       "negation                   165\n",
       "num_to_verbal              165\n",
       "orthography_starts_with    165\n",
       "rhymes                     165\n",
       "second_word_letter         165\n",
       "sentence_similarity        165\n",
       "sentiment                  165\n",
       "singular_to_plural         165\n",
       "sum                        165\n",
       "synonyms                   165\n",
       "taxonomy_animal            165\n",
       "translation_en-de          165\n",
       "translation_en-es          165\n",
       "translation_en-fr          165\n",
       "word_in_context            165\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"task\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_results = samples.groupby('task').agg(\n",
    "    mean=('evaluation', 'mean'),\n",
    "    q25=('evaluation', lambda x: x.quantile(0.25)),\n",
    "    q50=('evaluation', lambda x: x.quantile(0.50)),  # Same as median\n",
    "    q75=('evaluation', lambda x: x.quantile(0.75)),\n",
    "    q100=('evaluation', lambda x: x.quantile(1.00))\n",
    ").reset_index()\n",
    "\n",
    "len(task_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>mean</th>\n",
       "      <th>q25</th>\n",
       "      <th>q50</th>\n",
       "      <th>q75</th>\n",
       "      <th>q100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>active_to_passive</td>\n",
       "      <td>0.498462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antonyms</td>\n",
       "      <td>0.074574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diff</td>\n",
       "      <td>0.331053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first_word_letter</td>\n",
       "      <td>0.136107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>larger_animal</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>letters_list</td>\n",
       "      <td>0.241986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negation</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_to_verbal</td>\n",
       "      <td>0.428086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>orthography_starts_with</td>\n",
       "      <td>0.164283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rhymes</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>second_word_letter</td>\n",
       "      <td>0.033753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sentence_similarity</td>\n",
       "      <td>0.120356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.094441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>singular_to_plural</td>\n",
       "      <td>0.267567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sum</td>\n",
       "      <td>0.182097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>synonyms</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>taxonomy_animal</td>\n",
       "      <td>0.298644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>translation_en-de</td>\n",
       "      <td>0.074291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>translation_en-es</td>\n",
       "      <td>0.056964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>translation_en-fr</td>\n",
       "      <td>0.101938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>0.028324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       task      mean       q25       q50       q75  q100\n",
       "0         active_to_passive  0.498462  0.000000  0.500000  0.823529  1.00\n",
       "1                  antonyms  0.074574  0.000000  0.000000  0.000000  1.00\n",
       "2                      diff  0.331053  0.000000  0.285714  0.400000  1.00\n",
       "3         first_word_letter  0.136107  0.000000  0.000000  0.166667  1.00\n",
       "4             larger_animal  0.349500  0.000000  0.285714  0.666667  1.00\n",
       "5              letters_list  0.241986  0.000000  0.000000  0.571429  1.00\n",
       "6                  negation  0.521000  0.266667  0.588235  0.800000  1.00\n",
       "7             num_to_verbal  0.428086  0.000000  0.500000  0.800000  1.00\n",
       "8   orthography_starts_with  0.164283  0.000000  0.181818  0.222222  1.00\n",
       "9                    rhymes  0.254800  0.000000  0.000000  0.400000  1.00\n",
       "10       second_word_letter  0.033753  0.000000  0.000000  0.000000  0.40\n",
       "11      sentence_similarity  0.120356  0.000000  0.000000  0.222222  1.00\n",
       "12                sentiment  0.094441  0.000000  0.000000  0.000000  1.00\n",
       "13       singular_to_plural  0.267567  0.000000  0.000000  0.333333  1.00\n",
       "14                      sum  0.182097  0.000000  0.000000  0.333333  1.00\n",
       "15                 synonyms  0.002424  0.000000  0.000000  0.000000  0.40\n",
       "16          taxonomy_animal  0.298644  0.000000  0.363636  0.500000  0.75\n",
       "17        translation_en-de  0.074291  0.000000  0.000000  0.000000  1.00\n",
       "18        translation_en-es  0.056964  0.000000  0.000000  0.000000  1.00\n",
       "19        translation_en-fr  0.101938  0.000000  0.000000  0.000000  1.00\n",
       "20          word_in_context  0.028324  0.000000  0.000000  0.000000  0.40"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
